## 날짜: 2024-02-25

### 스크럼
- 딥다이브를 통해 이번 주 학습 내용을 복습하고 문제를 해결한다.
- 사실 이 날 뭐했는지 기억이 안나 딥다이브 주제로 대신 한다. 

### 새로 배운 내용

## 손실 함수(MSE)의 정의와 수치화 방식
- MSE(Mean Squared Error, 평균 제곱 오차)는 예측값과 실제값의 차이를 제곱하여 평균을 구하는 방식이다.
- 수식: MSE = (1/n) * Σ (예측값 - 실제값)²
- 예측값과 실제값 간의 차이를 제곱하여 오차가 클수록 큰 값으로 반영한다.
- 주로 회귀 모델에서 손실 함수로 사용된다.

## Gradient Descent와 Adam 옵티마이저 비교
- **Gradient Descent**
  - 손실 함수의 기울기를 계산하여 가중치를 업데이트하는 방법
  - 학습률 설정이 중요하며, 너무 크면 발산하고 너무 작으면 수렴 속도가 느려진다.
  - 종류: Batch, Stochastic, Mini-batch Gradient Descent
- **Adam 옵티마이저**
  - Gradient Descent에 Momentum과 RMSprop을 결합한 방식
  - 적응적 학습률을 제공하여 빠르고 안정적인 학습 가능
  - 장점: 학습 속도가 빠르고 최적화가 안정적임
  - 단점: 하이퍼파라미터 조정이 필요하고, 특정 경우 일반 Gradient Descent보다 성능이 낮을 수 있음

## ANN의 XOR 문제 해결 방법
- 단순한 Perceptron은 선형 분리가 불가능하여 XOR 문제를 해결할 수 없다.
- 다층 퍼셉트론(MLP)을 사용하여 비선형 활성화 함수를 적용하면 해결 가능하다.
- 은닉층을 추가하고 ReLU, sigmoid 등의 활성화 함수를 사용하여 비선형성을 도입한다.

## Fully Connected Neural Network와 CNN의 차이점
- **Fully Connected Neural Network (FCNN)**
  - 모든 뉴런이 이전 계층의 모든 뉴런과 연결된 구조
  - 주로 구조화된 데이터 처리에 사용됨
- **Convolutional Neural Network (CNN)**
  - 합성곱 계층을 사용하여 이미지의 공간적 구조를 유지하면서 특징을 추출
  - 주요 구성 요소: 합성곱 계층(Convolutional Layer), 풀링 계층(Pooling Layer), 완전 연결 계층(Fully Connected Layer)
  - 이미지 처리 및 패턴 인식에 강점이 있음

## Backpropagation을 통한 가중치 조정 과정
- 오차 역전파(Backpropagation)는 손실 함수를 최소화하기 위해 가중치를 조정하는 방법이다.
- 과정
  1. 순전파(Forward Propagation): 입력값이 네트워크를 통해 출력값으로 변환됨
  2. 손실 계산: 예측값과 실제값의 차이를 기반으로 손실 계산
  3. 역전파(Backward Propagation): 손실 함수를 미분하여 각 가중치에 대한 그래디언트 계산
  4. 가중치 업데이트: Gradient Descent 등을 사용하여 가중치 조정
  5. 반복 수행하여 학습 진행

## CNN의 합성곱 계층과 풀링 계층의 역할
- **합성곱 계층(Convolutional Layer)**
  - 필터(커널)를 사용하여 입력 데이터의 특징을 추출
  - 지역적인 패턴을 학습하여 공간적 관계 유지
  - 주요 연산: 합성곱 연산, 활성화 함수 적용
- **풀링 계층(Pooling Layer)**
  - 다운샘플링을 통해 데이터 크기를 줄이고 계산량을 감소시킴
  - 대표적인 방식: 최대 풀링(Max Pooling), 평균 풀링(Average Pooling)
  - 노이즈 제거 및 중요한 특징 강조

### 오늘의 도전 과제와 해결 방법
- **도전 과제 1:** 경사 하강법의 학습률 설정 문제
  - 해결 방법: 학습률 스케줄링 적용 및 적응적 옵티마이저(AdaGrad, Adam 등) 활용
- **도전 과제 2:** CNN 모델 학습 시 과적합 발생
  - 해결 방법: 데이터 증강(Data Augmentation), 드롭아웃(Dropout) 사용, 정규화 기법 적용

### 오늘의 회고
- MSE와 경사 하강법을 복습하며 손실 함수의 중요성을 다시 인식하였다.  
- XOR 문제 해결을 위한 MLP 구조와 비선형성 도입의 필요성을 이해하였다.  
- CNN의 합성곱 계층과 풀링 계층이 어떻게 특징을 추출하는지 학습하며 이미지 처리의 원리를 정리하였다.  
- 앞으로 CNN과 RNN을 결합한 모델에 대해 더 깊이 연구할 계획이다.

### 참고 자료 및 링크
- [경사 하강법 이해](https://en.wikipedia.org/wiki/Gradient_descent)
- [Adam 옵티마이저 개념](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)