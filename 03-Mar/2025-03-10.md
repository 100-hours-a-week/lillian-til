## 날짜: 2025-03-10  

### 스크럼  
- 자연어 처리(NLP) 기본 개념 학습  
- 순환 신경망(RNN) 및 LSTM 구조 이해  
- Transformer 모델 및 GAN 개념 학습  

### 새로 배운 내용  
#### 자연어 처리(Natural Language Processing, NLP)  
- 기계가 인간의 언어를 이해하고 처리할 수 있도록 하는 기술  
- 토큰화, 어휘 임베딩, 문장 분류, 번역, 감성 분석 등에 활용  

#### 순환 신경망(Recurrent Neural Network, RNN)  
- 순차적인 데이터를 처리하는 신경망 구조  
- 과거 정보를 기억하고, 시계열 데이터 및 자연어 처리에 강점이 있음  
- 하지만 긴 문맥을 학습할 때 기울기 소실(Vanishing Gradient) 문제가 발생  

#### 장기 기억 신경망(Long Short-Term Memory, LSTM)  
- RNN의 기울기 소실 문제를 해결하기 위해 등장  
- 셀 상태(Cell State)와 게이트 메커니즘(입력, 삭제, 출력 게이트)을 활용하여 장기 기억 가능  
- 챗봇, 번역, 시계열 예측 등 다양한 NLP 작업에 활용  

#### Text CNN  
- CNN을 텍스트 데이터에 적용하여 특징을 추출하는 방법  
- 합성곱 필터를 사용하여 문장의 의미적 패턴을 학습  
- 감성 분석, 문장 분류 등의 태스크에서 효과적  

#### 생성적 적대 신경망(Generative Adversarial Network, GAN)  
- 생성자(Generator)와 판별자(Discriminator)가 경쟁하며 학습하는 모델  
- 데이터 생성을 위한 신경망으로, 이미지 생성, 텍스트 생성 등 다양한 분야에서 활용  
- NLP에서는 텍스트 생성 및 데이터 증강에 응용  

#### Transformer 모델  
- RNN, LSTM을 대체하는 강력한 NLP 모델 구조  
- 셀프 어텐션(Self-Attention) 메커니즘을 활용하여 병렬 연산 가능  
- 대표적인 모델: BERT(Bidirectional Encoder Representations from Transformers), GPT(Generative Pre-trained Transformer)  

### 추가 미션  
- LSTM과 Transformer의 성능 비교 실험  
- Text CNN을 활용한 문장 분류 모델 구현  

### 오늘의 회고  
- 자연어 처리에서 RNN과 LSTM의 차이점을 명확히 이해할 수 있었다.  
- Transformer 모델이 왜 현대 NLP에서 중요한 역할을 하는지 배울 수 있었다.  
- GAN의 개념을 NLP에 적용하는 방법이 흥미로웠으며, 추가적으로 실습해보고 싶다.  

### 참고 자료 및 링크  
- [RNN 개념](https://en.wikipedia.org/wiki/Recurrent_neural_network)  
- [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory)  
- [Transformer](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture))  
